:PROPERTIES:
:Author: chaosmosis
:Score: 2
:DateUnix: 1480483815.0
:DateShort: 2016-Nov-30
:END:

From skimming /Superintelligence/ and the majority of the Sequences, my understanding is that Eliezer's and Nick Bostrom's predictions about AI risk overlap in like ninety-five percent of the particulars. Bostrom is highly credible and well regarded in the academic community, unlike Yudkowsky. If you think their opinions on AI are fundamentally flawed, you should make an argument of some sort to that effect. Don't just assert they're obviously wrong.

And this probably isn't the place for that, anyway.