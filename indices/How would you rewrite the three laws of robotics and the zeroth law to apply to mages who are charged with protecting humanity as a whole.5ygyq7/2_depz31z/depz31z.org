:PROPERTIES:
:Author: UndeadBBQ
:Score: 2
:DateUnix: 1489088362.0
:DateShort: 2017-Mar-09
:END:

For those who are unsure what those are:

- Zeroth law: A robot may not harm humanity, or, by inaction, allow humanity to come to harm

- First: A robot may not injure a human being or, through inaction, allow a human being to come to harm.

- Second: A robot must obey the orders given it by human beings except where such orders would conflict with the First Law.

- Third: A robot must protect its own existence as long as such protection does not conflict with the First or Second Laws

To rewrite them for some sort of mage-unit, maybe this is a good start. As I would presume that such an organisation would be more charitable, I assume they include sentients in their sphere of protection.

- 0: A Mage may not harm the societies of sentients, or, by inaction, allow the societies of sentients to come to harm.

However, here we have the main problem. Depending on their morals this can mean a variety of things. Are they Utilitarianists? Are they Kantists? Are they Marxists? Or something completely different? The question is "What does harm to sentient societies, and what doesn't?" How do they decide between them?

- 1: A mage may not injure a sentient being or, through inaction, allow a human being to come to harm.

- 2: A mage must obey the orders of his superior, except where such orders would conflict with the zeroth and first law.

- 3: A mage must seek to prolong, guard and improve his existence as long as such actions do not conflict with the zeroth, first or second Law

This third one is also interesting. The robotic law says that the robot must remain functioning. I would argue that in order to remain functioning, a sentient being must seek to progress. At least humans do not do well in ever-same repetition. How this law would apply to your mages, I don't know.

Also, as some already said: the Laws written by Asimov are made for machines.