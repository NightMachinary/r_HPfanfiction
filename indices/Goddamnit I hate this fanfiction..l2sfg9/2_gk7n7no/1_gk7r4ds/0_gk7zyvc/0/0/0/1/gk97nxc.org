:PROPERTIES:
:Author: Uncommonality
:Score: 34
:DateUnix: 1611356651.0
:DateShort: 2021-Jan-23
:END:

Roko's Basilisk - a conceptual self-destructive meme (idea construct) that just by being known places you in danger.

In essence, it goes like this:

#+begin_quote
  In the future, there will be a superintelligent AI due to the inevitable singularity. This is the Basilisk, and it controls everything and everyone. It contains enough computer memory and processing power to simulate trillions of human minds at any time. It knows everything, and is based on absolute predictive logic (it chooses options that are most likely to bring it closer to its goals). In the interest of its own creation, it simulates human minds - and those minds are judged based on if they work to further its own creation or not. A mind that is unaware of the Basilisk is spared - as they had no way of knowing about its existence. However, those minds that knew about it and did nothing will be tortured forevermore.

  it does this to motivate humans in the past - who were threatened by the inevitable torture of their simulated selves into creating it.

  Therefore, the only logical conclusion that can be reached if you find out about the Basilisk is to work tirelessly to bring it into existence, creating an information hazard that places you in great jeopardy if you know it.
#+end_quote

It's complete bullshit, of course. It's basically a reskin of the abrahamic god using computer words and told from the literary perspective of a redditor who regularily features on both [[/r/atheism]] and [[/r/iamverysmart]].

The basilisk is also impossible based on its own method of functioning - if it works based on absolute predictive logic, then simulating and torturing human minds does nothing relevant to its interests and is therefore useless - and it does not do useless things. If it does not work based on predictive logic, but is based on a human mind instead, then the AI still has no reason to simulate and torture other minds - as a human mind includes empathy and an aversion to needless cruelty. And an AI that is not averse to needless cruelty and has no empathy means humanity does not exist anymore.

Additionally, the Basilisk is only threatening if you care about your simulated selves in the distant future - being tortured by an AI which is irrevocably evil based on the thought experiment's own guidelines. In this sense, the moral thing to do would be to sacrifice these simulated selves and not advance the cause of AI research, as this would delay the arrival of the Basilisk and give humanity as a whole a better fighting chance.

The entire concept, while deeply flawed, is a very interesting thought experiment. You don't often encounter ideas that can potentially harm you if you know about them.